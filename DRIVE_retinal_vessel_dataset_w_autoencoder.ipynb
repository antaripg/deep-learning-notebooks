{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMdZ6AukjtV7w8sBJakjcO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antaripg/deep-learning-notebooks/blob/main/DRIVE_retinal_vessel_dataset_w_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZRu1H8RKHHf"
      },
      "source": [
        "### Checking for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CgvsFJpbJuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb704300-6247-4b92-b874-a8cb278ea551"
      },
      "source": [
        " gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, \n",
            "and then re-execute this cell.\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"\n",
            "menu, and then select High-RAM in the Runtime shape dropdown. Then, \n",
            "re-execute this cell.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkFEAzBmKL24"
      },
      "source": [
        "# Mounting the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdQ7i6AYKEKX",
        "outputId": "67190414-1418-435d-c6b6-91063f29a6c6"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZilcvBNnKPlP"
      },
      "source": [
        "### Importing the necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9SRi8RtKGF4",
        "outputId": "703bd32d-935c-4a17-c282-cb0ab7bf45ff"
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7OUS_VvKfUq"
      },
      "source": [
        "Labelpath = '/content/drive/MyDrive/DRIVE_Retinal_dataset/training/1st_manual/'\n",
        "Datapath = '/content/drive/MyDrive/DRIVE_Retinal_dataset/training/images/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O81orrF79c3J"
      },
      "source": [
        "### Crop Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cF8tXyl9gF6"
      },
      "source": [
        "# crop random 10 x 10 patches from images and also crop corresponding label\n",
        "\n",
        "def img_transfer(img,imgLabel, bh, bw, no_of_patch):\n",
        "\n",
        "  h = img.shape[0]\n",
        "  w = img.shape[1]\n",
        "  c = img.shape[2]\n",
        "  ImgArr = np.empty((no_of_patch, bh*bw*3))\n",
        "  LabelArr = np.empty((no_of_patch, bh*bw*1))\n",
        "\n",
        "  for i in range(no_of_patch):\n",
        "    ih = random.randint(0, h-bh)\n",
        "    iw = random.randint(0, w-bw)\n",
        "    iArrI = img[ih:ih+bh,iw:iw+bw,:]\n",
        "    iArrL = imgLabel[ih:ih+bh,iw:iw+bw,:]\n",
        "    for ci in range(c):\n",
        "      for bhi in range(bh):\n",
        "        for bwi in range(bw):\n",
        "          ImgArr[i][ci*bh*bw + bhi*bw + bwi] = iArrI[bhi][bwi][ci]\n",
        "          if ci ==0:\n",
        "            LabelArr[i][ci*bh*bw + bhi*bw + bwi] = iArrL[bhi][bwi][ci]\n",
        "\n",
        "    return ImgArr,LabelArr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXn0TbsvAntW"
      },
      "source": [
        "patchH = 10 # height of the patch\n",
        "patchW = 10 # width of the patch\n",
        "PatchperImage = 1000 # no of patches per image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs7P34ioBAJF"
      },
      "source": [
        "TrainImages = torch.DoubleTensor(13*PatchperImage, 3*patchH*patchW)\n",
        "TrainLabels = torch.LongTensor(13*PatchperImage, patchH*patchW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahnS1593BSMm",
        "outputId": "22bb028d-c43a-4a14-f27f-688020ad72d8"
      },
      "source": [
        "# Read each training image and crop random patches\n",
        "\n",
        "t_no = 0\n",
        "for img_no in range(13):\n",
        "    imD = Image.open(Datapath + str(img_no+21) + '_training.tif')\n",
        "    imD = np.array(imD)\n",
        "\n",
        "    imL = Image.open(Labelpath + str(img_no+21) + '_manual1.gif')\n",
        "    imL = np.array(imL)\n",
        "    imL = np.reshape(imL, (imL.shape[0],imL.shape[1],1))\n",
        "\n",
        "    imD,imL = img_transfer(imD,imL, patchH, patchW, PatchperImage)\n",
        "    imD = imD/255.0\n",
        "    imL = imL/255.0\n",
        "    for i in range(PatchperImage):\n",
        "        TrainImages[t_no] = torch.from_numpy(imD[i])\n",
        "        TrainLabels[t_no] = torch.from_numpy(imL[i])\n",
        "        t_no = t_no + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHxPwJIPDHTf",
        "outputId": "622c0ad6-b652-4c1c-8ed3-8677b29d0081"
      },
      "source": [
        "# Printing Training Image and Labels size\n",
        "\n",
        "print(TrainImages.size())\n",
        "print(TrainLabels.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([13000, 300])\n",
            "torch.Size([13000, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPqLgnQYDT-C"
      },
      "source": [
        "### Availability of GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c59eIExADP5b",
        "outputId": "11349d44-075c-4201-b6f6-1938967ed922"
      },
      "source": [
        "# Checking availability of GPU\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "  print(\"GPU is available!\")\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  print(\"GPU not available!\")\n",
        "  device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU not available!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hasrefo7D_8J"
      },
      "source": [
        "### Define the AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "TkHHA5VtECPr",
        "outputId": "f3eab285-2c59-48eb-a82c-a584be023bec"
      },
      "source": [
        "class autoencoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(autoencoder, self).__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(patchH*patchW*3, patchH*patchW),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(patchH*patchW, patchH*patchW),\n",
        "        nn.Tanh())\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(patchH*patchW, patchH*patchW),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(patchH*patchW, patchH*patchW*3),\n",
        "        nn.Sigmoid())\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "\n",
        "net = autoencoder()\n",
        "print(net)\n",
        "\n",
        "net = net.double().to(device)\n",
        "reset = False\n",
        "if reset:\n",
        "  for layer in net.children():\n",
        "    if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "else:\n",
        "  pass\n",
        "\n",
        "init_weights = copy.deepcopy(net.encoder[0].weight.data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "autoencoder(\n",
            "  (encoder): Sequential(\n",
            "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (3): Tanh()\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=100, out_features=300, bias=True)\n",
            "    (3): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-81f031f5c6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    }
  ]
}